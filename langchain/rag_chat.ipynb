{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nOjGL5RZyNrK"
      ],
      "mount_file_id": "1qQP2-wd0661kJDafLSl0pJCCHvizMl56",
      "authorship_tag": "ABX9TyMUKAG909N3qEUSwZhwGfqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjdzliu/ai_lab/blob/main/langchain/rag_chat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain+OpenAI+vector  \n",
        "Retrival Augmented Generation  \n",
        "优势：  \n",
        "增加了回答专业领域内的知识  \n",
        "知识库可以频繁更新  \n",
        "知识库内容变更可以追踪  \n",
        "数据隐私\n",
        "\n",
        "这里包含了4个例子:  \n",
        "\n",
        "\n",
        "1.   Try a sample no-RAG\n",
        "2.   提示工程的方式\n",
        "3.   知识库内容存放在一个pdf文件中\n",
        "4.   使用Huggingface的embedding模型\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iqx8q39sL0XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try a sample no-RAG"
      ],
      "metadata": {
        "id": "iAROP3wsxR5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install langchain openapi chromadb   cohere tiktoken langchain_openai"
      ],
      "metadata": {
        "id": "A6wtzSAjR-Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "apikey=userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "chat = ChatOpenAI(\n",
        "    openai_api_key=apikey,\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "vvQOi9ZVXXf2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "messages=[\n",
        "    SystemMessage(content=\"You are a helpful assistant\"),\n",
        "    HumanMessage(content=\"Knock Knock\"),\n",
        "    AIMessage(content=\"who is there\"),\n",
        "    HumanMessage(content=\"Orange\")\n",
        "]"
      ],
      "metadata": {
        "id": "sEeWUAySZSM0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res=chat.invoke(messages)"
      ],
      "metadata": {
        "id": "iCoYIN05w8Zb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdHmyYh4xCKx",
        "outputId": "a5d5be6b-c386-42ee-cb64-f8868d7a0502"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Orange who?')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages.append(res)\n",
        "res=chat.invoke(messages)"
      ],
      "metadata": {
        "id": "qmQbKY4bxHZf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDoSc7aAxNeS",
        "outputId": "32fc966e-4a06-41e0-982d-8594f56e72d4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='是的，我知道Baichuan2模型。Baichuan2是由百川智能开发的一系列开源可商用的大规模预训练语言模型。Baichuan2-7B-Base和Baichuan2-13B-Base是其其中两个模型，它们是基于2.6T高质量多语言数据进行训练的。这些模型在保留了上一代开源模型的生成与创作能力、流畅的多轮对话能力以及部署门槛较低等特性的基础上，还在数学、代码、安全、逻辑推理、语义理解等能力上有显著提升。')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 提示工程的方式"
      ],
      "metadata": {
        "id": "nOjGL5RZyNrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "baichuan2_information=[\n",
        "    \"百川智能在北京召开大模型发布会,正式发布Baichuan2开源大模型,昇腾AI基础软硬件平台正式支持Baichuan2大模型,并在昇思MindSpore开源社区大模型平台上线Baichuan2-7B模型开放体验。\",\n",
        "    \"Baichuan2-7B-Base和 Baichuan2-13B-Base,均基于2.6T⾼质量多语⾔数据进⾏训练,在保留了上一代开源模型良好的生成与创作能力,流畅的多轮对话能力以及部署⻔槛较低等众多特性的基础上,两个模型在数学、代码、安全、逻辑推理、语义理解等能⼒有显著提升\",\n",
        "    \"Baichuan2大模型是由百川智能开发的一系列开源可商用的大规模预训练语言模型\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "CvtWIsvJyUbA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_knowledge='\\n'.join(baichuan2_information)"
      ],
      "metadata": {
        "id": "_qE6GgsVzd59"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(source_knowledge)"
      ],
      "metadata": {
        "id": "sZ_1pmzmzmA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"你知道baichuan2模型吗\""
      ],
      "metadata": {
        "id": "l2ZrspN0zqFu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template=f\"\"\"基于以下内容回答问题:\n",
        "内容:\n",
        "{source_knowledge}\n",
        "Query:\n",
        "{query}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_RITcUeIzvLQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=HumanMessage(\n",
        "    content=prompt_template\n",
        ")\n",
        "messages.append(prompt)\n",
        "res=chat(messages)"
      ],
      "metadata": {
        "id": "74DgWNy00tGh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "VD0ccK6206CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 知识库内容存放在一个pdf文件中\n"
      ],
      "metadata": {
        "id": "Y4PcmAGb6JcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pypdf"
      ],
      "metadata": {
        "id": "Ro8KZpIR6IUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader"
      ],
      "metadata": {
        "id": "DfcQ_Oqx6Y0r"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=PyPDFLoader(\"https://arxiv.org/pdf/2309.10305.pdf\")"
      ],
      "metadata": {
        "id": "UBaIz9F96Y5q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages=loader.load_and_split()"
      ],
      "metadata": {
        "id": "PIXQWyagMfEZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pages[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0YpkH0bMfHV",
        "outputId": "75a5a38d-935d-4436-d897-85fb795cc38c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='However, most open-source large language\\nmodels have focused primarily on English. For\\ninstance, the main data source for LLaMA\\nis Common Crawl1, which comprises 67% of\\nLLaMA’s pre-training data but is filtered to English\\ncontent only. Other open source LLMs such as\\nMPT (MosaicML, 2023) and Falcon (Penedo et al.,\\n2023) are also focused on English and have limited\\ncapabilities in other languages. This hinders the\\ndevelopment and application of LLMs in specific\\nlanguages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,\\nBaichuan 2 achieves significant improvements over\\nBaichuan 1. On general benchmarks like MMLU\\n(Hendrycks et al., 2021a), CMMLU (Li et al.,\\n2023), and C-Eval (Huang et al., 2023), Baichuan\\n2-7B achieves nearly 30% higher performance\\ncompared to Baichuan 1-7B. Specifically, Baichuan\\n2 is optimized to improve performance on math\\nand code problems. On the GSM8K (Cobbe\\net al., 2021) and HumanEval (Chen et al., 2021)\\nevaluations, Baichuan 2 nearly doubles the results\\nof the Baichuan 1. In addition, Baichuan 2 also\\ndemonstrates strong performance on medical and\\nlegal domain tasks. On benchmarks such as\\nMedQA (Jin et al., 2021) and JEC-QA (Zhong\\net al., 2020), Baichuan 2 outperforms other open-\\nsource models, making it a suitable foundation\\nmodel for domain-specific optimization.\\nAdditionally, we also released two chat\\nmodels, Baichuan 2-7B-Chat and Baichuan 2-\\n13B-Chat, optimized to follow human instructions.\\nThese models excel at dialogue and context\\nunderstanding. We will elaborate on our\\napproaches to improve the safety of Baichuan 2.\\nBy open-sourcing these models, we hope to enable\\nthe community to further improve the safety of\\nlarge language models, facilitating more research\\non responsible LLMs development.\\nFurthermore, in spirit of research collaboration\\nand continuous improvement, we are also releasing\\nthe checkpoints of Baichuan 2 at various stages\\n1https://commoncrawl.org/of training from 200 billion tokens up to the full\\n2.6 trillion tokens. We found that even for the 7\\nbillion parameter model, performance continued to\\nimprove after training on more than 2.6 trillion\\ntokens. By sharing these intermediary results,\\nwe hope to provide the community with greater\\ninsight into the training dynamics of Baichuan 2.\\nUnderstanding these dynamics is key to unraveling\\nthe inner working mechanism of large language\\nmodels (Biderman et al., 2023a; Tirumala et al.,\\n2022). We believe the release of these checkpoints\\nwill pave the way for further advances in this\\nrapidly developing field.\\nIn this technical report, we will also share\\nsome of the trials, errors, and lessons learned\\nthrough training Baichuan 2. In the following\\nsections, we will present detailed modifications\\nmade to the vanilla Transformer architecture and\\nour training methodology. We will then describe\\nour fine-tuning methods to align the foundation\\nmodel with human preferences. Finally, we will\\nbenchmark the performance of our models against\\nother LLMs on a set of standard tests. Throughout\\nthe report, we aim to provide transparency into\\nour process, including unsuccessful experiments,\\nto advance collective knowledge in developing\\nLLMs. Baichuan 2’s foundation models and\\nchat models are available for both research and\\ncommercial use at https://github.com/\\nbaichuan-inc/Baichuan2\\n2 Pre-training\\nThis section introduces the training procedure\\nfor the Baichuan 2 foundation models. Before\\ndiving into the model details, we first show the\\noverall performance of the Baichuan 2 base models\\ncompared to other open or closed-sourced models' metadata={'source': 'https://arxiv.org/pdf/2309.10305.pdf', 'page': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter=RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "docs=text_splitter.split_documents(pages)"
      ],
      "metadata": {
        "id": "4UJ6hhDGMfKv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "LSDV-v5OMfOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "利用embeding模型对每个文本进行向量化    \n",
        "使用OpenAI的embed_model,也需要提供一个openai_api_key\n"
      ],
      "metadata": {
        "id": "aNwVUmarOL7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embed_model=OpenAIEmbeddings(openai_api_key=apikey)\n",
        "vectorstore=Chroma.from_documents(documents=docs,embedding=embed_model,collection_name=\"openai_embed\")\n",
        "\n"
      ],
      "metadata": {
        "id": "XF8pPAcjMfRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"How large is the baichuan2 voculary\"\n",
        "result=vectorstore.similarity_search(query,k=2)"
      ],
      "metadata": {
        "id": "bKykx9pzMF1j"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "BlxCw1sPMF5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 封装一个构造prompt的方法\n",
        "def augment_promp(query:str):\n",
        "  results = vectorstore.similarity_search(query,k=3)\n",
        "  source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
        "  augmented_promt=f\"\"\"\n",
        "  Using the contexts below, answer the query.\n",
        "  context:\n",
        "  {source_knowledge}\n",
        "  query:\n",
        "  {query}\n",
        "  \"\"\"\n",
        "  return augmented_promt"
      ],
      "metadata": {
        "id": "QmIxv7pRMcBC"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(augment_promp(query))"
      ],
      "metadata": {
        "id": "fcg3YAWWTWc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 用简单的chat方式，想llm提问\n",
        "prompt=HumanMessage(\n",
        "    content=augment_promp(query)\n",
        ")\n",
        "messages_rag=[\n",
        "    prompt\n",
        "]\n",
        "es=chat.invoke(messages_rag)\n",
        "print(es)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdcJNsUQUSfP",
        "outputId": "0e755166-5031-44b7-e9bc-e708377b4b1b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The vocabulary size of Baichuan 2 is 125,696.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 使用Huggingface的embedding模型"
      ],
      "metadata": {
        "id": "nbVNPh9yVG1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install sentence-transformers"
      ],
      "metadata": {
        "id": "jcbFpc-4VzxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "model_name=\"jinaai/jina-embeddings-v2-small-en\""
      ],
      "metadata": {
        "id": "IkYdMNgwVVGe"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用huggingface的model，添加认证\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "k-vbKo3HhvMz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "1LEUn7_ghwX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=HuggingFaceEmbeddings(model_name=model_name)"
      ],
      "metadata": {
        "id": "dWckAE3xVVJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore_hf=Chroma.from_documents(documents=docs,embedding=embedding,collection_name=\"huggingface_embed2\")"
      ],
      "metadata": {
        "id": "dBTf5ae5V9Lv"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=vectorstore_hf.similarity_search(query,k=2)"
      ],
      "metadata": {
        "id": "uVMsHXBMWUzO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpwpG45EgI_W",
        "outputId": "9a423e37-8bf4-4df3-eb55-196d607a246c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='Baichuan 2: Open Large-scale Language Models\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan\\nDian Wang, Dong Yan, Fan Yang, Fei Deng, Feng Wang, Feng Liu, Guangwei Ai\\nGuosheng Dong, Haizhou Zhao, Hang Xu, Haoze Sun, Hongda Zhang, Hui Liu, Jiaming Ji\\nJian Xie, Juntao Dai, Kun Fang, Lei Su, Liang Song, Lifeng Liu, Liyun Ru, Luyao Ma\\nMang Wang, Mickel Liu, MingAn Lin, Nuolan Nie, Peidong Guo, Ruiyang Sun', metadata={'page': 0, 'source': 'https://arxiv.org/pdf/2309.10305.pdf'}), Document(page_content='languages, such as Chinese.\\nIn this technical report, we introduce Baichuan\\n2, a series of large-scale multilingual language\\nmodels. Baichuan 2 has two separate models,\\nBaichuan 2-7B with 7 billion parameters and\\nBaichuan 2-13B with 13 billion parameters. Both\\nmodels were trained on 2.6 trillion tokens, which\\nto our knowledge is the largest to date, more than\\ndouble that of Baichuan 1 (Baichuan, 2023b,a).\\nWith such a massive amount of training data,', metadata={'page': 1, 'source': 'https://arxiv.org/pdf/2309.10305.pdf'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_hf=HumanMessage(\n",
        "    content=augment_promp(query)\n",
        ")\n",
        "messages_rag_hf=[\n",
        "    prompt\n",
        "]\n",
        "es=chat.invoke(messages_rag_hf)\n",
        "print(es)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCba1whhjakt",
        "outputId": "f479fd81-8035-4345-e02c-93137e6d917f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The vocabulary size of Baichuan 2 is 125,696.'\n"
          ]
        }
      ]
    }
  ]
}